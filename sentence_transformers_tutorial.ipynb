{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentence Transformers Tutorial\n",
        "\n",
        "This notebook will teach you how to use sentence transformers with practical examples using the text files in our project.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Setup and Installation](#setup)\n",
        "2. [Loading Text Files](#loading)\n",
        "3. [Basic Sentence Embeddings](#basic)\n",
        "4. [Semantic Search](#search)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation {#setup}\n",
        "\n",
        "First, let's install the required packages if needed and import all necessary libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment and run this cell if you need to install packages\n",
        "\n",
        "# Install uv first (if needed)\n",
        "# !curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "\n",
        "# Install dependencies with uv\n",
        "# !uv sync\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading Text Files {#loading}\n",
        "\n",
        "Let's load our sample text files and prepare them for analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_text_files(directory='sample_texts'):\n",
        "    \"\"\"\n",
        "    Load all text files from a directory\n",
        "    Returns a dictionary with filename as key and content as value\n",
        "    \"\"\"\n",
        "    texts = {}\n",
        "    \n",
        "    if os.path.exists(directory):\n",
        "        for filename in os.listdir(directory):\n",
        "            if filename.endswith('.txt'):\n",
        "                filepath = os.path.join(directory, filename)\n",
        "                with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                    content = file.read()\n",
        "                    # Remove .txt extension from key\n",
        "                    key = filename.replace('.txt', '')\n",
        "                    texts[key] = content\n",
        "    \n",
        "    return texts\n",
        "\n",
        "# Load our text files\n",
        "documents = load_text_files()\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents:\")\n",
        "for key in documents.keys():\n",
        "    print(f\"- {key}\")\n",
        "    print(f\"  Preview: {documents[key][:100]}...\\\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_into_sentences(text):\n",
        "    \"\"\"\n",
        "    Simple sentence splitter\n",
        "    \"\"\"\n",
        "    import re\n",
        "    # Split on periods, exclamation marks, and question marks\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    # Clean up and filter out empty sentences\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return sentences\n",
        "\n",
        "# Create sentence-level data\n",
        "sentence_data = []\n",
        "for topic, content in documents.items():\n",
        "    sentences = split_into_sentences(content)\n",
        "    for sentence in sentences:\n",
        "        sentence_data.append({\n",
        "            'topic': topic,\n",
        "            'sentence': sentence\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame for easier handling\n",
        "df_sentences = pd.DataFrame(sentence_data)\n",
        "print(f\"Total sentences: {len(df_sentences)}\")\n",
        "print(\"\\\\nSample sentences by topic:\")\n",
        "for topic in df_sentences['topic'].unique():\n",
        "    sample = df_sentences[df_sentences['topic'] == topic].iloc[0]['sentence']\n",
        "    print(f\"{topic}: {sample}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Basic Sentence Embeddings {#basic}\n",
        "\n",
        "Let's start with the basics - loading a sentence transformer model and creating embeddings:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a pre-trained sentence transformer model\n",
        "# We'll use 'all-MiniLM-L6-v2' - it's fast and effective for most tasks\n",
        "print(\"Loading sentence transformer model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(f\"Model loaded! Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create embeddings for our documents\n",
        "print(\"Creating document embeddings...\")\n",
        "doc_texts = list(documents.values())\n",
        "doc_names = list(documents.keys())\n",
        "\n",
        "# Generate embeddings\n",
        "doc_embeddings = model.encode(doc_texts)\n",
        "\n",
        "print(f\"Created embeddings for {len(doc_embeddings)} documents\")\n",
        "print(f\"Each embedding has {doc_embeddings[0].shape[0]} dimensions\")\n",
        "print(f\"Embedding shape: {doc_embeddings.shape}\")\n",
        "\n",
        "# Create sentence embeddings\n",
        "print(\"\\\\nCreating sentence embeddings...\")\n",
        "sentences = df_sentences['sentence'].tolist()\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "print(f\"Created embeddings for {len(sentence_embeddings)} sentences\")\n",
        "\n",
        "# Add embeddings to our DataFrame\n",
        "df_sentences['embedding'] = list(sentence_embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Semantic Search {#search}\n",
        "\n",
        "Let's implement a semantic search function that finds the most relevant sentences:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def semantic_search(query, df_sentences, model, top_k=5):\n",
        "    \"\"\"\n",
        "    Perform semantic search on sentences\n",
        "    \"\"\"\n",
        "    # Encode the query\n",
        "    query_embedding = model.encode([query])\n",
        "    \n",
        "    # Calculate similarities\n",
        "    sentence_embeddings = np.vstack(df_sentences['embedding'].values)\n",
        "    similarities = cosine_similarity(query_embedding, sentence_embeddings)[0]\n",
        "    \n",
        "    # Add similarities to dataframe\n",
        "    df_results = df_sentences.copy()\n",
        "    df_results['similarity'] = similarities\n",
        "    \n",
        "    # Sort by similarity and return top results\n",
        "    top_results = df_results.nlargest(top_k, 'similarity')\n",
        "    \n",
        "    return top_results[['topic', 'sentence', 'similarity']]\n",
        "\n",
        "# Test semantic search\n",
        "search_queries = [\n",
        "    \"artificial intelligence and neural networks\",\n",
        "    \"pasta and italian recipes\", \n",
        "    \"solo travel adventures\",\n",
        "    \"startup and entrepreneurship\",\n",
        "    \"climate change research\"\n",
        "]\n",
        "\n",
        "for query in search_queries:\n",
        "    print(f\"\\\\nüîç Searching for: '{query}'\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    results = semantic_search(query, df_sentences, model, top_k=3)\n",
        "    \n",
        "    for idx, row in results.iterrows():\n",
        "        print(f\"üìÑ [{row['topic']}] (Score: {row['similarity']:.3f})\")\n",
        "        print(f\"   {row['sentence']}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, you've learned:\n",
        "\n",
        "1. **Basic Usage**: How to load models and create embeddings\n",
        "2. **Text Loading**: Working with multiple text files\n",
        "3. **Semantic Search**: Finding relevant content based on meaning\n",
        "4. **Practical Applications**: Real-world examples with different topics\n",
        "\n",
        "### Next Steps:\n",
        "- Experiment with different models for your specific use case\n",
        "- Try fine-tuning models on your domain-specific data\n",
        "- Explore clustering and visualization techniques\n",
        "- Build more sophisticated applications using these fundamentals\n",
        "\n",
        "### Key Takeaways:\n",
        "- Sentence transformers convert text to meaningful vector representations\n",
        "- Cosine similarity measures semantic closeness between texts\n",
        "- Different models have different strengths and computational requirements\n",
        "- Embeddings enable powerful applications like search, clustering, and QA systems\n",
        "\n",
        "To run this notebook:\n",
        "1. Install dependencies: `uv sync`\n",
        "2. Start Jupyter: `jupyter notebook` or `jupyter lab`\n",
        "3. Open `sentence_transformers_tutorial.ipynb`\n",
        "4. Run cells sequentially from top to bottom\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
