{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformers Tutorial\n",
    "\n",
    "This notebook will teach you how to use sentence transformers with practical examples using the text files in our project.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Loading Text Files](#loading)\n",
    "3. [Basic Sentence Embeddings](#basic)\n",
    "4. [Semantic Search](#search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Text Files {#loading}\n",
    "\n",
    "Let's load our sample text files and prepare them for analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 documents:\n",
      "- cooking\n",
      "  Preview: Italian cuisine is renowned worldwide for its rich flavors and traditional recipes. Pasta dishes lik...\n",
      "\n",
      "- science\n",
      "  Preview: Quantum physics explores the behavior of matter and energy at the smallest scales. Unlike classical ...\n",
      "\n",
      "- travel\n",
      "  Preview: European travel offers incredible diversity within relatively short distances. From the romantic can...\n",
      "\n",
      "- programming\n",
      "  Preview: Python is a versatile programming language known for its simplicity and readability. It's widely use...\n",
      "\n",
      "- business\n",
      "  Preview: Entrepreneurship involves identifying market opportunities and creating innovative solutions to meet...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_text_files(directory='sample_texts'):\n",
    "    \"\"\"\n",
    "    Load all text files from a directory\n",
    "    Returns a dictionary with filename as key and content as value\n",
    "    \"\"\"\n",
    "    texts = {}\n",
    "    \n",
    "    if os.path.exists(directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.txt'):\n",
    "                filepath = os.path.join(directory, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    # Remove .txt extension from key\n",
    "                    key = filename.replace('.txt', '')\n",
    "                    texts[key] = content\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Load our text files\n",
    "documents = load_text_files()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents:\")\n",
    "for key in documents.keys():\n",
    "    print(f\"- {key}\")\n",
    "    print(f\"  Preview: {documents[key][:100]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 43\n",
      "\n",
      "Sample sentences by topic:\n",
      "cooking: Italian cuisine is renowned worldwide for its rich flavors and traditional recipes\n",
      "science: Quantum physics explores the behavior of matter and energy at the smallest scales\n",
      "travel: European travel offers incredible diversity within relatively short distances\n",
      "programming: Python is a versatile programming language known for its simplicity and readability\n",
      "business: Entrepreneurship involves identifying market opportunities and creating innovative solutions to meet consumer needs\n"
     ]
    }
   ],
   "source": [
    "def split_into_sentences(text):\n",
    "    \"\"\"\n",
    "    Simple sentence splitter\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Split on periods, exclamation marks, and question marks\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    # Clean up and filter out empty sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# Create sentence-level data\n",
    "sentence_data = []\n",
    "for topic, content in documents.items():\n",
    "    sentences = split_into_sentences(content)\n",
    "    for sentence in sentences:\n",
    "        sentence_data.append({\n",
    "            'topic': topic,\n",
    "            'sentence': sentence\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df_sentences = pd.DataFrame(sentence_data)\n",
    "print(f\"Total sentences: {len(df_sentences)}\")\n",
    "print(\"\\nSample sentences by topic:\")\n",
    "for topic in df_sentences['topic'].unique():\n",
    "    sample = df_sentences[df_sentences['topic'] == topic].iloc[0]['sentence']\n",
    "    print(f\"{topic}: {sample}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Sentence Embeddings {#basic}\n",
    "\n",
    "Let's start with the basics - loading a sentence transformer model and creating embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence transformer model...\n",
      "Model loaded! Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained sentence transformer model\n",
    "# We'll use 'all-MiniLM-L6-v2' - it's fast and effective for most tasks\n",
    "print(\"Loading sentence transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"Model loaded! Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating document embeddings...\n",
      "Created embeddings for 5 documents\n",
      "Each embedding has 384 dimensions\n",
      "Embedding shape: (5, 384)\n",
      "\n",
      "Creating sentence embeddings...\n",
      "Created embeddings for 43 sentences\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for our documents\n",
    "print(\"Creating document embeddings...\")\n",
    "doc_texts = list(documents.values())\n",
    "doc_names = list(documents.keys())\n",
    "\n",
    "# Generate embeddings\n",
    "doc_embeddings = model.encode(doc_texts)\n",
    "\n",
    "print(f\"Created embeddings for {len(doc_embeddings)} documents\")\n",
    "print(f\"Each embedding has {doc_embeddings[0].shape[0]} dimensions\")\n",
    "print(f\"Embedding shape: {doc_embeddings.shape}\")\n",
    "\n",
    "# Create sentence embeddings\n",
    "print(\"\\nCreating sentence embeddings...\")\n",
    "sentences = df_sentences['sentence'].tolist()\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "print(f\"Created embeddings for {len(sentence_embeddings)} sentences\")\n",
    "\n",
    "# Add embeddings to our DataFrame\n",
    "df_sentences['embedding'] = list(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Search {#search}\n",
    "\n",
    "Let's implement a semantic search function that finds the most relevant sentences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for: 'artificial intelligence and neural networks'\n",
      "============================================================\n",
      "üìÑ [science] (Score: 0.581)\n",
      "   Deep learning, a subset of AI, uses neural networks inspired by the human brain to solve complex problems\n",
      "\n",
      "üìÑ [science] (Score: 0.509)\n",
      "   Artificial intelligence represents one of the most significant technological advances of our time\n",
      "\n",
      "üìÑ [science] (Score: 0.427)\n",
      "   Machine learning algorithms can now recognize patterns, make predictions, and even generate creative content\n",
      "\n",
      "\n",
      "üîç Searching for: 'pasta and italian recipes'\n",
      "============================================================\n",
      "üìÑ [cooking] (Score: 0.678)\n",
      "   Italian cuisine is renowned worldwide for its rich flavors and traditional recipes\n",
      "\n",
      "üìÑ [cooking] (Score: 0.666)\n",
      "   Pasta dishes like spaghetti carbonara, fettuccine alfredo, and lasagna have become staples in kitchens around the globe\n",
      "\n",
      "üìÑ [cooking] (Score: 0.615)\n",
      "   The secret to authentic Italian cooking lies in using fresh, high-quality ingredients\n",
      "\n",
      "\n",
      "üîç Searching for: 'solo travel adventures'\n",
      "============================================================\n",
      "üìÑ [travel] (Score: 0.746)\n",
      "   Solo travel empowers individuals to explore destinations at their own pace and according to their personal interests\n",
      "\n",
      "üìÑ [travel] (Score: 0.715)\n",
      "   Planning solo trips requires more preparation but offers unparalleled freedom and self-discovery opportunities\n",
      "\n",
      "üìÑ [travel] (Score: 0.605)\n",
      "   Many solo travelers report increased confidence, independence, and cultural awareness after their journeys\n",
      "\n",
      "\n",
      "üîç Searching for: 'startup and entrepreneurship'\n",
      "============================================================\n",
      "üìÑ [business] (Score: 0.649)\n",
      "   Entrepreneurship involves identifying market opportunities and creating innovative solutions to meet consumer needs\n",
      "\n",
      "üìÑ [business] (Score: 0.633)\n",
      "   The startup ecosystem provides resources like accelerators, venture capital, and mentorship programs to support new ventures\n",
      "\n",
      "üìÑ [business] (Score: 0.632)\n",
      "   Successful entrepreneurs combine vision, determination, and strategic thinking to build sustainable businesses\n",
      "\n",
      "\n",
      "üîç Searching for: 'climate change research'\n",
      "============================================================\n",
      "üìÑ [science] (Score: 0.754)\n",
      "   Climate science studies the Earth's climate system and how it changes over time\n",
      "\n",
      "üìÑ [science] (Score: 0.585)\n",
      "   This scientific field is crucial for predicting future climate scenarios and developing mitigation strategies\n",
      "\n",
      "üìÑ [science] (Score: 0.570)\n",
      "   Researchers analyze temperature data, ice core samples, and atmospheric conditions to understand global warming patterns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def semantic_search(query, df_sentences, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform semantic search on sentences\n",
    "    \"\"\"\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sentence_embeddings = np.vstack(df_sentences['embedding'].values)\n",
    "    similarities = cosine_similarity(query_embedding, sentence_embeddings)[0]\n",
    "    \n",
    "    # Add similarities to dataframe\n",
    "    df_results = df_sentences.copy()\n",
    "    df_results['similarity'] = similarities\n",
    "    \n",
    "    # Sort by similarity and return top results\n",
    "    top_results = df_results.nlargest(top_k, 'similarity')\n",
    "    \n",
    "    return top_results[['topic', 'sentence', 'similarity']]\n",
    "\n",
    "# Test semantic search\n",
    "search_queries = [\n",
    "    \"artificial intelligence and neural networks\",\n",
    "    \"pasta and italian recipes\", \n",
    "    \"solo travel adventures\",\n",
    "    \"startup and entrepreneurship\",\n",
    "    \"climate change research\"\n",
    "]\n",
    "\n",
    "for query in search_queries:\n",
    "    print(f\"\\nüîç Searching for: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = semantic_search(query, df_sentences, model, top_k=3)\n",
    "    \n",
    "    for idx, row in results.iterrows():\n",
    "        print(f\"üìÑ [{row['topic']}] (Score: {row['similarity']:.3f})\")\n",
    "        print(f\"   {row['sentence']}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you've learned:\n",
    "\n",
    "1. **Basic Usage**: How to load models and create embeddings\n",
    "2. **Text Loading**: Working with multiple text files\n",
    "3. **Semantic Search**: Finding relevant content based on meaning\n",
    "4. **Practical Applications**: Real-world examples with different topics\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different models for your specific use case\n",
    "- Try fine-tuning models on your domain-specific data\n",
    "- Explore clustering and visualization techniques\n",
    "- Build more sophisticated applications using these fundamentals\n",
    "\n",
    "### Key Takeaways:\n",
    "- Sentence transformers convert text to meaningful vector representations\n",
    "- Cosine similarity measures semantic closeness between texts\n",
    "- Different models have different strengths and computational requirements\n",
    "- Embeddings enable powerful applications like search, clustering, and QA systems\n",
    "\n",
    "To run this notebook:\n",
    "1. Install dependencies: `uv pip install -r requirements.txt`\n",
    "2. Start Jupyter: `jupyter notebook` or `jupyter lab`\n",
    "3. Open `sentence_transformers_tutorial.ipynb`\n",
    "4. Run cells sequentially from top to bottom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformers Tutorial\n",
    "\n",
    "This notebook will teach you how to use sentence transformers with practical examples using the text files in our project.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Loading Text Files](#loading)\n",
    "3. [Basic Sentence Embeddings](#basic)\n",
    "4. [Text Similarity Analysis](#similarity)\n",
    "5. [Semantic Search](#search)\n",
    "6. [Document Clustering](#clustering)\n",
    "7. [Visualization](#visualization)\n",
    "8. [Advanced Examples](#advanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Text Files {#loading}\n",
    "\n",
    "Let's load our sample text files and prepare them for analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_files(directory='sample_texts'):\n",
    "    \"\"\"\n",
    "    Load all text files from a directory\n",
    "    Returns a dictionary with filename as key and content as value\n",
    "    \"\"\"\n",
    "    texts = {}\n",
    "    \n",
    "    if os.path.exists(directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.txt'):\n",
    "                filepath = os.path.join(directory, filename)\n",
    "                with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    # Remove .txt extension from key\n",
    "                    key = filename.replace('.txt', '')\n",
    "                    texts[key] = content\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Load our text files\n",
    "documents = load_text_files()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents:\")\n",
    "for key in documents.keys():\n",
    "    print(f\"- {key}\")\n",
    "    print(f\"  Preview: {documents[key][:100]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also split the documents into sentences for more granular analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    \"\"\"\n",
    "    Simple sentence splitter\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Split on periods, exclamation marks, and question marks\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    # Clean up and filter out empty sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# Create sentence-level data\n",
    "sentence_data = []\n",
    "for topic, content in documents.items():\n",
    "    sentences = split_into_sentences(content)\n",
    "    for sentence in sentences:\n",
    "        sentence_data.append({\n",
    "            'topic': topic,\n",
    "            'sentence': sentence\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df_sentences = pd.DataFrame(sentence_data)\n",
    "print(f\"Total sentences: {len(df_sentences)}\")\n",
    "print(\"\\nSample sentences by topic:\")\n",
    "for topic in df_sentences['topic'].unique():\n",
    "    sample = df_sentences[df_sentences['topic'] == topic].iloc[0]['sentence']\n",
    "    print(f\"{topic}: {sample}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Sentence Embeddings {#basic}\n",
    "\n",
    "Let's start with the basics - loading a sentence transformer model and creating embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained sentence transformer model\n",
    "# We'll use 'all-MiniLM-L6-v2' - it's fast and effective for most tasks\n",
    "print(\"Loading sentence transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"Model loaded! Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for our documents\n",
    "print(\"Creating document embeddings...\")\n",
    "doc_texts = list(documents.values())\n",
    "doc_names = list(documents.keys())\n",
    "\n",
    "# Generate embeddings\n",
    "doc_embeddings = model.encode(doc_texts)\n",
    "\n",
    "print(f\"Created embeddings for {len(doc_embeddings)} documents\")\n",
    "print(f\"Each embedding has {doc_embeddings[0].shape[0]} dimensions\")\n",
    "print(f\"Embedding shape: {doc_embeddings.shape}\")\n",
    "\n",
    "# Create sentence embeddings\n",
    "print(\"\\nCreating sentence embeddings...\")\n",
    "sentences = df_sentences['sentence'].tolist()\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "print(f\"Created embeddings for {len(sentence_embeddings)} sentences\")\n",
    "\n",
    "# Add embeddings to our DataFrame\n",
    "df_sentences['embedding'] = list(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Search {#search}\n",
    "\n",
    "Let's implement a semantic search function that finds the most relevant sentences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, df_sentences, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform semantic search on sentences\n",
    "    \"\"\"\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sentence_embeddings = np.vstack(df_sentences['embedding'].values)\n",
    "    similarities = cosine_similarity(query_embedding, sentence_embeddings)[0]\n",
    "    \n",
    "    # Add similarities to dataframe\n",
    "    df_results = df_sentences.copy()\n",
    "    df_results['similarity'] = similarities\n",
    "    \n",
    "    # Sort by similarity and return top results\n",
    "    top_results = df_results.nlargest(top_k, 'similarity')\n",
    "    \n",
    "    return top_results[['topic', 'sentence', 'similarity']]\n",
    "\n",
    "# Test semantic search\n",
    "search_queries = [\n",
    "    \"artificial intelligence and neural networks\",\n",
    "    \"pasta and italian recipes\", \n",
    "    \"solo travel adventures\",\n",
    "    \"startup and entrepreneurship\",\n",
    "    \"climate change research\"\n",
    "]\n",
    "\n",
    "for query in search_queries:\n",
    "    print(f\"\\nüîç Searching for: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = semantic_search(query, df_sentences, model, top_k=3)\n",
    "    \n",
    "    for idx, row in results.iterrows():\n",
    "        print(f\"üìÑ [{row['topic']}] (Score: {row['similarity']:.3f})\")\n",
    "        print(f\"   {row['sentence']}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you've learned:\n",
    "\n",
    "1. **Basic Usage**: How to load models and create embeddings\n",
    "2. **Text Loading**: Working with multiple text files\n",
    "3. **Semantic Search**: Finding relevant content based on meaning\n",
    "4. **Practical Applications**: Real-world examples with different topics\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different models for your specific use case\n",
    "- Try fine-tuning models on your domain-specific data\n",
    "- Explore clustering and visualization techniques\n",
    "- Build more sophisticated applications using these fundamentals\n",
    "\n",
    "### Key Takeaways:\n",
    "- Sentence transformers convert text to meaningful vector representations\n",
    "- Cosine similarity measures semantic closeness between texts\n",
    "- Different models have different strengths and computational requirements\n",
    "- Embeddings enable powerful applications like search, clustering, and QA systems\n",
    "\n",
    "To run this notebook:\n",
    "1. Install dependencies: `uv pip install -r requirements.txt`\n",
    "2. Start Jupyter: `jupyter notebook` or `jupyter lab`\n",
    "3. Open `sentence_transformers_tutorial.ipynb`\n",
    "4. Run cells sequentially from top to bottom\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
